{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e560ae",
   "metadata": {},
   "source": [
    "Import all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea805bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in d:\\adni data\\venv\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: numpy in d:\\adni data\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: scipy in d:\\adni data\\venv\\lib\\site-packages (1.15.2)\n",
      "Requirement already satisfied: matplotlib in d:\\adni data\\venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: scikit-image in d:\\adni data\\venv\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: torch in d:\\adni data\\venv\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in d:\\adni data\\venv\\lib\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: pandas in d:\\adni data\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: packaging>=20 in d:\\adni data\\venv\\lib\\site-packages (from nibabel) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\adni data\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\adni data\\venv\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in d:\\adni data\\venv\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\adni data\\venv\\lib\\site-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\adni data\\venv\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: filelock in d:\\adni data\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\adni data\\venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: jinja2 in d:\\adni data\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\adni data\\venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\adni data\\venv\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\adni data\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\adni data\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\adni data\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\adni data\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\adni data\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\adni data\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nibabel numpy scipy matplotlib scikit-image torch torchvision pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441e9ad",
   "metadata": {},
   "source": [
    "GPU Available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8138e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6b15e",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc058292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0) \n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "#Arquitectura VGG16\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "#Cargar Datos\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Entrenar el modelo\n",
    "model = VGG3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    print(f\"Test Acc: {acc_val:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vgg16_3d_alzheimer.pth\")\n",
    "print(\"Modelo guardado como vgg16_3d_alzheimer.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6766b",
   "metadata": {},
   "source": [
    "Entrenar mas epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b570643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ADNI Data\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|██████████| 208/208 [1:16:30<00:00, 22.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 681.4067, Train Acc: 41.03%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 208/208 [1:16:04<00:00, 21.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 674.0343, Train Acc: 41.67%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 208/208 [1:16:01<00:00, 21.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 673.7220, Train Acc: 41.83%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 208/208 [1:15:55<00:00, 21.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 672.0051, Train Acc: 41.35%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 208/208 [1:15:42<00:00, 21.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 673.2498, Train Acc: 40.38%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 208/208 [1:15:50<00:00, 21.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 671.3271, Train Acc: 39.90%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 208/208 [1:15:40<00:00, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 670.2091, Train Acc: 41.67%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 208/208 [1:15:34<00:00, 21.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 666.1644, Train Acc: 41.67%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 208/208 [1:15:47<00:00, 21.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 665.4625, Train Acc: 39.58%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 208/208 [1:15:44<00:00, 21.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 661.7003, Train Acc: 41.67%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 208/208 [1:15:38<00:00, 21.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Loss: 664.5823, Train Acc: 41.51%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 208/208 [1:15:40<00:00, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Loss: 661.3610, Train Acc: 41.19%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 208/208 [1:15:40<00:00, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Loss: 660.9245, Train Acc: 41.19%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 208/208 [1:15:42<00:00, 21.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Loss: 648.1820, Train Acc: 41.51%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 208/208 [1:15:49<00:00, 21.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Loss: 641.0342, Train Acc: 41.03%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 208/208 [1:15:49<00:00, 21.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Loss: 635.2856, Train Acc: 41.51%\n",
      "Test Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 208/208 [1:15:52<00:00, 21.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Loss: 633.5754, Train Acc: 40.87%\n",
      "Test Acc: 44.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 208/208 [1:15:44<00:00, 21.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Loss: 645.4486, Train Acc: 39.90%\n",
      "Test Acc: 31.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 208/208 [1:15:51<00:00, 21.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Loss: 641.4108, Train Acc: 43.75%\n",
      "Test Acc: 38.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 208/208 [1:15:48<00:00, 21.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Loss: 640.0389, Train Acc: 44.55%\n",
      "Test Acc: 42.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 208/208 [1:15:49<00:00, 21.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Loss: 633.2382, Train Acc: 44.71%\n",
      "Test Acc: 46.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 208/208 [1:15:44<00:00, 21.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Loss: 631.7136, Train Acc: 44.23%\n",
      "Test Acc: 45.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 208/208 [1:15:56<00:00, 21.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Loss: 635.8697, Train Acc: 45.99%\n",
      "Test Acc: 46.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 208/208 [1:15:53<00:00, 21.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Loss: 637.6892, Train Acc: 45.51%\n",
      "Test Acc: 41.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 208/208 [1:17:01<00:00, 22.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Loss: 628.8421, Train Acc: 45.51%\n",
      "Test Acc: 44.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 208/208 [1:17:41<00:00, 22.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Loss: 625.3900, Train Acc: 46.63%\n",
      "Test Acc: 45.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 208/208 [1:17:28<00:00, 22.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Loss: 634.6051, Train Acc: 45.35%\n",
      "Test Acc: 43.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 208/208 [1:17:19<00:00, 22.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Loss: 635.4264, Train Acc: 44.23%\n",
      "Test Acc: 43.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 208/208 [1:17:47<00:00, 22.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Loss: 625.4521, Train Acc: 46.79%\n",
      "Test Acc: 43.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 208/208 [1:17:37<00:00, 22.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Loss: 623.6009, Train Acc: 45.67%\n",
      "Test Acc: 45.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 208/208 [1:17:29<00:00, 22.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Loss: 619.7074, Train Acc: 46.47%\n",
      "Test Acc: 44.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 208/208 [1:17:48<00:00, 22.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Loss: 625.7944, Train Acc: 47.44%\n",
      "Test Acc: 47.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 208/208 [1:18:16<00:00, 22.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Loss: 627.5965, Train Acc: 46.15%\n",
      "Test Acc: 50.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 208/208 [1:18:15<00:00, 22.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Loss: 615.4534, Train Acc: 49.36%\n",
      "Test Acc: 45.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 208/208 [1:18:21<00:00, 22.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Loss: 611.2115, Train Acc: 48.88%\n",
      "Test Acc: 29.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 208/208 [1:17:58<00:00, 22.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Loss: 616.4426, Train Acc: 49.20%\n",
      "Test Acc: 44.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 208/208 [1:18:26<00:00, 22.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Loss: 604.5189, Train Acc: 48.08%\n",
      "Test Acc: 45.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 208/208 [1:18:20<00:00, 22.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Loss: 596.1348, Train Acc: 46.79%\n",
      "Test Acc: 48.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 208/208 [1:18:10<00:00, 22.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Loss: 590.8724, Train Acc: 49.68%\n",
      "Test Acc: 51.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 208/208 [1:18:18<00:00, 22.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Loss: 581.3521, Train Acc: 51.92%\n",
      "Test Acc: 51.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 208/208 [1:18:37<00:00, 22.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Loss: 578.1206, Train Acc: 53.69%\n",
      "Test Acc: 53.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 208/208 [1:18:32<00:00, 22.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Loss: 558.6744, Train Acc: 51.92%\n",
      "Test Acc: 55.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 208/208 [1:18:00<00:00, 22.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Loss: 563.8572, Train Acc: 53.04%\n",
      "Test Acc: 57.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 208/208 [1:17:49<00:00, 22.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Loss: 558.4570, Train Acc: 54.17%\n",
      "Test Acc: 48.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 208/208 [1:18:25<00:00, 22.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Loss: 547.0810, Train Acc: 53.85%\n",
      "Test Acc: 49.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 208/208 [1:18:53<00:00, 22.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Loss: 531.8274, Train Acc: 55.29%\n",
      "Test Acc: 57.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 208/208 [1:18:41<00:00, 22.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Loss: 524.8597, Train Acc: 57.85%\n",
      "Test Acc: 58.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 208/208 [1:18:32<00:00, 22.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Loss: 538.7532, Train Acc: 56.41%\n",
      "Test Acc: 52.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 208/208 [1:18:32<00:00, 22.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Loss: 503.5320, Train Acc: 58.81%\n",
      "Test Acc: 56.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 208/208 [1:18:33<00:00, 22.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Loss: 500.8329, Train Acc: 59.29%\n",
      "Test Acc: 57.89%\n",
      "Modelo guardado como vgg16_3d_alzheimer3.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 50\n",
    "LR = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0) \n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "#Arquitectura VGG16\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "#Cargar Datos\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Entrenar el modelo\n",
    "model = VGG3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-8, verbose=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    print(f\"Test Acc: {acc_val:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vgg16_3d_alzheimer3.pth\")\n",
    "print(\"Modelo guardado como vgg16_3d_alzheimer3.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87149e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ADNI Data\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado. Continuando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 208/208 [59:06<00:00, 17.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 - Train Loss: 0.6169, Train Acc: 74.84%\n",
      "🔍 Val Loss: 0.7007, Val Acc: 66.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 208/208 [1:02:09<00:00, 17.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 - Train Loss: 0.5712, Train Acc: 74.52%\n",
      "🔍 Val Loss: 0.6204, Val Acc: 70.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 208/208 [1:02:34<00:00, 18.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 - Train Loss: 0.6025, Train Acc: 75.48%\n",
      "🔍 Val Loss: 0.6550, Val Acc: 70.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 208/208 [1:02:56<00:00, 18.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 - Train Loss: 0.5857, Train Acc: 74.36%\n",
      "🔍 Val Loss: 0.6438, Val Acc: 67.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 208/208 [1:04:00<00:00, 18.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 - Train Loss: 0.5495, Train Acc: 75.96%\n",
      "🔍 Val Loss: 0.6116, Val Acc: 72.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 208/208 [1:03:38<00:00, 18.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 - Train Loss: 0.5327, Train Acc: 76.28%\n",
      "🔍 Val Loss: 0.6577, Val Acc: 70.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 208/208 [1:03:27<00:00, 18.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 - Train Loss: 0.5280, Train Acc: 75.80%\n",
      "🔍 Val Loss: 0.6632, Val Acc: 69.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 208/208 [1:02:25<00:00, 18.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 - Train Loss: 0.5575, Train Acc: 77.24%\n",
      "🔍 Val Loss: 0.6512, Val Acc: 73.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 208/208 [1:02:45<00:00, 18.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 - Train Loss: 0.4859, Train Acc: 79.33%\n",
      "🔍 Val Loss: 0.5839, Val Acc: 74.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 208/208 [1:03:32<00:00, 18.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 - Train Loss: 0.4382, Train Acc: 82.37%\n",
      "🔍 Val Loss: 0.6163, Val Acc: 72.73%\n",
      "💾 Modelo actualizado guardado como vgg16_3d_alzheimer4_cont.pth\n"
     ]
    }
   ],
   "source": [
    "# Script para continuar entrenamiento VGG16 3D cargando pesos anteriores\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 10  # ← entrenar 30 épocas más\n",
    "LR = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ========= DATASET =========\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0)  # [1, D, H, W]\n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "# ========= MODELO =========\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(64, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(128, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(256, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ========= DATOS =========\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ========= CARGAR MODELO =========\n",
    "model = VGG3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"vgg16_3d_alzheimer3_cont.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-8, verbose=True)\n",
    "\n",
    "print(\"✅ Modelo cargado. Continuando entrenamiento...\")\n",
    "\n",
    "# ========= ENTRENAMIENTO =========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "    print(f\"✅ Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_correct, val_loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    print(f\"🔍 Val Loss: {avg_val_loss:.4f}, Val Acc: {acc_val:.2f}%\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "# ========= GUARDAR =========\n",
    "torch.save(model.state_dict(), \"vgg16_3d_alzheimer4_cont.pth\")\n",
    "print(\"💾 Modelo actualizado guardado como vgg16_3d_alzheimer4_cont.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cf8971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ADNI Data\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado. Continuando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 208/208 [1:03:10<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 - Train Loss: 0.5002, Train Acc: 77.56%\n",
      "🔍 Val Loss: 0.5524, Val Acc: 77.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 208/208 [1:02:35<00:00, 18.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 - Train Loss: 0.4840, Train Acc: 80.93%\n",
      "🔍 Val Loss: 0.5868, Val Acc: 75.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 208/208 [1:03:01<00:00, 18.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 - Train Loss: 0.5158, Train Acc: 76.92%\n",
      "🔍 Val Loss: 0.7603, Val Acc: 64.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 208/208 [1:02:46<00:00, 18.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 - Train Loss: 0.4866, Train Acc: 80.77%\n",
      "🔍 Val Loss: 0.7037, Val Acc: 68.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 208/208 [1:02:51<00:00, 18.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 - Train Loss: 0.3958, Train Acc: 84.29%\n",
      "🔍 Val Loss: 0.5954, Val Acc: 74.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 208/208 [1:02:44<00:00, 18.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 - Train Loss: 0.3682, Train Acc: 85.26%\n",
      "🔍 Val Loss: 0.7735, Val Acc: 66.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 208/208 [1:02:46<00:00, 18.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 - Train Loss: 0.3567, Train Acc: 84.46%\n",
      "🔍 Val Loss: 0.5498, Val Acc: 77.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 208/208 [1:02:25<00:00, 18.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 - Train Loss: 0.3676, Train Acc: 84.46%\n",
      "🔍 Val Loss: 0.5203, Val Acc: 82.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 208/208 [1:02:57<00:00, 18.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 - Train Loss: 0.3634, Train Acc: 85.42%\n",
      "🔍 Val Loss: 0.5044, Val Acc: 79.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 208/208 [1:02:58<00:00, 18.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 - Train Loss: 0.3465, Train Acc: 86.70%\n",
      "🔍 Val Loss: 0.4983, Val Acc: 83.25%\n",
      "💾 Modelo actualizado guardado como vgg16_3d_alzheimer4_cont.pth\n"
     ]
    }
   ],
   "source": [
    "# Script para continuar entrenamiento VGG16 3D cargando pesos anteriores\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 10  # ← entrenar 30 épocas más\n",
    "LR = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ========= DATASET =========\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0)  # [1, D, H, W]\n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "# ========= MODELO =========\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(64, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(128, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(256, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ========= DATOS =========\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ========= CARGAR MODELO =========\n",
    "model = VGG3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"vgg16_3d_alzheimer4_cont.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-8, verbose=True)\n",
    "\n",
    "print(\"✅ Modelo cargado. Continuando entrenamiento...\")\n",
    "\n",
    "# ========= ENTRENAMIENTO =========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "    print(f\"✅ Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_correct, val_loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    print(f\"🔍 Val Loss: {avg_val_loss:.4f}, Val Acc: {acc_val:.2f}%\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "# ========= GUARDAR =========\n",
    "torch.save(model.state_dict(), \"vgg16_3d_alzheimer5_cont.pth\")\n",
    "print(\"💾 Modelo actualizado guardado como vgg16_3d_alzheimer4_cont.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f0b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ADNI Data\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado. Continuando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 208/208 [1:01:37<00:00, 17.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 - Train Loss: 0.3915, Train Acc: 83.97%\n",
      "🔍 Val Loss: 0.5895, Val Acc: 76.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 208/208 [1:01:53<00:00, 17.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 - Train Loss: 0.3975, Train Acc: 84.62%\n",
      "🔍 Val Loss: 0.5193, Val Acc: 79.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 208/208 [1:01:43<00:00, 17.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 - Train Loss: 0.3517, Train Acc: 86.54%\n",
      "🔍 Val Loss: 0.6754, Val Acc: 73.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 208/208 [1:02:00<00:00, 17.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 - Train Loss: 0.4255, Train Acc: 82.69%\n",
      "🔍 Val Loss: 0.6415, Val Acc: 74.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 208/208 [1:01:57<00:00, 17.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 - Train Loss: 0.3297, Train Acc: 87.50%\n",
      "🔍 Val Loss: 0.5389, Val Acc: 78.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 208/208 [1:01:47<00:00, 17.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 - Train Loss: 0.2759, Train Acc: 90.22%\n",
      "🔍 Val Loss: 0.5106, Val Acc: 81.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 208/208 [1:01:45<00:00, 17.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 - Train Loss: 0.2277, Train Acc: 91.19%\n",
      "🔍 Val Loss: 0.5684, Val Acc: 82.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 208/208 [1:02:15<00:00, 17.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 - Train Loss: 0.2226, Train Acc: 91.19%\n",
      "🔍 Val Loss: 0.6007, Val Acc: 81.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 208/208 [1:02:12<00:00, 17.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 - Train Loss: 0.2336, Train Acc: 91.51%\n",
      "🔍 Val Loss: 0.5110, Val Acc: 81.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 208/208 [1:01:54<00:00, 17.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 - Train Loss: 0.1646, Train Acc: 94.87%\n",
      "🔍 Val Loss: 0.5148, Val Acc: 85.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 208/208 [1:01:40<00:00, 17.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 11 - Train Loss: 0.1436, Train Acc: 94.55%\n",
      "🔍 Val Loss: 0.5054, Val Acc: 83.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 208/208 [1:01:52<00:00, 17.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 12 - Train Loss: 0.1534, Train Acc: 94.71%\n",
      "🔍 Val Loss: 0.5573, Val Acc: 81.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 208/208 [1:01:47<00:00, 17.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 13 - Train Loss: 0.1673, Train Acc: 93.59%\n",
      "🔍 Val Loss: 0.5115, Val Acc: 83.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 208/208 [1:01:35<00:00, 17.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 14 - Train Loss: 0.1351, Train Acc: 95.03%\n",
      "🔍 Val Loss: 0.5158, Val Acc: 84.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 208/208 [1:01:37<00:00, 17.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 15 - Train Loss: 0.1227, Train Acc: 94.55%\n",
      "🔍 Val Loss: 0.5536, Val Acc: 83.25%\n",
      "💾 Modelo actualizado guardado como vgg16_3d_alzheimer6_cont.pth\n"
     ]
    }
   ],
   "source": [
    "# Script para continuar entrenamiento VGG16 3D cargando pesos anteriores\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 15  # ← entrenar 30 épocas más\n",
    "LR = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ========= DATASET =========\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0)  # [1, D, H, W]\n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "# ========= MODELO =========\n",
    "class VGG3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(64, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(128, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(256, 256, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(256, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2),\n",
    "\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.Conv3d(512, 512, 3, padding=1), nn.ReLU(True),\n",
    "            nn.MaxPool3d(2, 2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ========= DATOS =========\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ========= CARGAR MODELO =========\n",
    "model = VGG3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"vgg16_3d_alzheimer5_cont.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-8, verbose=True)\n",
    "\n",
    "print(\"✅ Modelo cargado. Continuando entrenamiento...\")\n",
    "\n",
    "# ========= ENTRENAMIENTO =========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "    print(f\"✅ Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_correct, val_loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    print(f\"🔍 Val Loss: {avg_val_loss:.4f}, Val Acc: {acc_val:.2f}%\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "# ========= GUARDAR =========\n",
    "torch.save(model.state_dict(), \"vgg16_3d_alzheimer6_cont.pth\")\n",
    "print(\"💾 Modelo actualizado guardado como vgg16_3d_alzheimer6_cont.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "venv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
