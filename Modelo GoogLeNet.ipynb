{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e560ae",
   "metadata": {},
   "source": [
    "Import all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nibabel numpy scipy matplotlib scikit-image torch torchvision pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441e9ad",
   "metadata": {},
   "source": [
    "GPU Available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8138e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6b15e",
   "metadata": {},
   "source": [
    "GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc058292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\"\n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "\n",
    "        volume = torch.tensor(volume).unsqueeze(0) \n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "        return volume, label\n",
    "\n",
    "# Arqquitectura InceptionV3\n",
    "class InceptionV3Block3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Conv3d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 48, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(48, 64, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(96, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.AvgPool3d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([\n",
    "            self.branch1(x),\n",
    "            self.branch2(x),\n",
    "            self.branch3(x),\n",
    "            self.branch4(x)\n",
    "        ], 1)\n",
    "\n",
    "#Arquitectura GoogLeNetV3\n",
    "class GoogLeNetV3_3D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.inception_a1 = InceptionV3Block3D(64)\n",
    "        self.inception_a2 = InceptionV3Block3D(224)\n",
    "        self.inception_a3 = InceptionV3Block3D(224)\n",
    "\n",
    "        self.reduction_a = nn.Sequential(\n",
    "            nn.Conv3d(224, 384, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.inception_b1 = InceptionV3Block3D(384)\n",
    "        self.inception_b2 = InceptionV3Block3D(224)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(224, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_a1(x)\n",
    "        x = self.inception_a2(x)\n",
    "        x = self.inception_a3(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_b1(x)\n",
    "        x = self.inception_b2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "#Cargar datos\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Enrenamiento del modelo\n",
    "model = GoogLeNetV3_3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    print(f\"Test Acc: {acc_val:.2f}%\")\n",
    "\n",
    "# ========= GUARDAR =========\n",
    "torch.save(model.state_dict(), \"googlenetv3_3d_alzheimer.pth\")\n",
    "print(\"Modelo guardado como googlenetv3_3d_alzheimer.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6766b",
   "metadata": {},
   "source": [
    "Entrenar mas epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b570643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import scipy.ndimage\n",
    "\n",
    "CSV_PATH = r\"D:/ADNI Data/BETandGroups.csv\" \n",
    "IMG_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5  \n",
    "START_EPOCH = 75 \n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Nifti3DDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_shape = IMG_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"Ruta\"]\n",
    "        label = self.df.loc[idx, \"Grupo\"]\n",
    "\n",
    "        volume = nib.load(path).get_fdata().astype(np.float32)\n",
    "        zoom_factors = [t / s for t, s in zip(self.target_shape, volume.shape)]\n",
    "        volume = scipy.ndimage.zoom(volume, zoom=zoom_factors, order=3)\n",
    "\n",
    "        volume = (volume - np.mean(volume)) / (np.std(volume) + 1e-8)\n",
    "        volume = np.clip(volume, -3, 3)\n",
    "        volume = torch.tensor(volume).unsqueeze(0)\n",
    "        label = torch.tensor(self.label_encoder.transform([label])[0])\n",
    "\n",
    "        return volume, label\n",
    "\n",
    "class GoogLeNet3D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GoogLeNet3D, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(3, stride=2, padding=1),                 \n",
    "            nn.Conv3d(64, 128, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(3, stride=2, padding=1),                  \n",
    "            nn.Conv3d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))                        \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(CSV_PATH) \n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['Grupo'])\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, stratify=df['labels'], random_state=42)\n",
    "train_dataset = Nifti3DDataset(df_train, le)\n",
    "val_dataset = Nifti3DDataset(df_val, le)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = GoogLeNet3D(num_classes=len(le.classes_)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(r\"D:\\ADNI Data/googlenet3d_alzheimer_extended3.pth\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(START_EPOCH, START_EPOCH + EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{START_EPOCH + EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc_train = 100 * train_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {train_loss:.4f}, Train Acc: {acc_train:.2f}%\")\n",
    "\n",
    "    # Validaci√≥n\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    acc_val = 100 * val_correct / len(val_dataset)\n",
    "    print(f\"Test Acc: {acc_val:.2f}%\")\n",
    "\n",
    "# ========= GUARDAR MODELO Y CHECKPOINT\n",
    "torch.save(model.state_dict(), \"googlenet3d_alzheimer_extended4.pth\")\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'googlenet3d_checkpoint_epoch80.pth')\n",
    "print(\"Modelo actualizado y checkpoint guardado.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
